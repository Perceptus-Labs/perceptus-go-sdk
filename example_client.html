<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Perceptus Robot Session Client</title>
  <style>
    /* ... your existing styles ... */
    #videoPreview {
      width: 320px; height: 240px;
      border: 1px solid #ccc;
      display: block; margin: 10px 0;
    }
    .message {
      margin: 5px 0;
      padding: 5px;
      border-radius: 4px;
    }
    .error {
      background-color: #ffeeee;
      color: #cc0000;
    }
    .received {
      background-color: #eeffee;
      color: #006600;
    }
    .sent {
      background-color: #eeeeff;
      color: #000066;
    }
    .info {
      background-color: #ffffee;
      color: #666600;
    }
    .status {
      padding: 5px;
      border-radius: 4px;
      display: inline-block;
      margin-bottom: 10px;
    }
    .connected {
      background-color: #eeffee;
      color: #006600;
    }
    .disconnected {
      background-color: #ffeeee;
      color: #cc0000;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Perceptus Robot Session Client</h1>
    <div id="status" class="status disconnected">Disconnected</div>
    <button id="connectBtn" onclick="connect()">Connect</button>
    <button id="disconnectBtn" onclick="disconnect()" disabled>Disconnect</button>
    <button id="pingBtn" onclick="sendPing()" disabled>Send Ping</button>
    <button id="stopBtn" onclick="stopSession()" disabled>Stop Session</button>

    <h3>Live Video Preview</h3>
    <!-- hidden video to capture from camera -->
    <video id="localVideo" autoplay playsinline style="display:none;"></video>
    <!-- display local camera preview -->
    <canvas id="localCanvas" style="width: 320px; height: 240px; border: 1px solid #ccc; display: block; margin: 10px 0;"></canvas>
    <!-- display server‐sent video frames -->
    <img id="videoPreview" src="" alt="Server video frame"/>

    <h3>Send Test Audio Data</h3>
    <button id="audioBtn" onclick="toggleAudio()" disabled>Start Audio Stream</button>
    <div id="audioStatus" style="margin: 10px 0; color: #666;">Audio Status: Not started</div>

    <h3>Configuration</h3>
    <select id="videoFreq">
      <option value="1s">1 second</option>
      <option value="5s">5 seconds</option>
      <option value="10s">10 seconds</option>
      <option value="30s" selected>30 seconds</option>
    </select>
    <button id="configBtn" onclick="sendConfig()" disabled>Update Config</button>

    <h3>Messages</h3>
    <button onclick="clearMessages()">Clear</button>
    <div id="messages"></div>
  </div>

  <script>
    let ws, pingInterval, audioRecorder, audioStream, audioTimer, videoTimer;

    function connect() {
      ws = new WebSocket('ws://localhost:8080/robot/session');
      ws.onopen = () => {
        updateStatus('Connected', true);
        // start ping
        sendPing();
        pingInterval = setInterval(sendPing, 3000);
      };
      ws.onmessage = ({ data }) => {
        const msg = JSON.parse(data);
        if (msg.type === 'video_frame') {
          document.getElementById('videoPreview').src = 'data:image/jpeg;base64,' + msg.data.image_b64;
        } else {
          addMessage(JSON.stringify(msg), 'received');
        }
      };
      ws.onclose = () => {
        updateStatus('Disconnected', false);
        clearInterval(pingInterval);
        stopMedia();
      };
      ws.onerror = err => addMessage('WS error: '+err, 'error');
      // enable controls
      document.getElementById('disconnectBtn').disabled =
      document.getElementById('pingBtn').disabled =
      document.getElementById('stopBtn').disabled =
      document.getElementById('audioBtn').disabled =
      document.getElementById('configBtn').disabled = false;
      document.getElementById('connectBtn').disabled = true;
      
      // start camera capture into hidden <video>
      navigator.mediaDevices.getUserMedia({ video: true })
        .then(stream => {
          const vid = document.getElementById('localVideo');
          vid.srcObject = stream;
          vid.onloadedmetadata = () => vid.play();
          
          // capture a frame at the configured interval
          const frequency = document.getElementById('videoFreq').value;
          const interval = parseDuration(frequency);
          videoTimer = setInterval(captureAndSendFrame, interval);
          addMessage(`Video capture started with frequency: ${frequency} (${interval}ms)`, 'info');
        })
        .catch(e => addMessage('Camera error: '+e, 'error'));
    }

    function captureAndSendFrame() {
      const vid = document.getElementById('localVideo');
      if (ws.readyState !== WebSocket.OPEN) return;
      const c = document.createElement('canvas');
      c.width = vid.videoWidth; c.height = vid.videoHeight;
      const ctx = c.getContext('2d');
      ctx.drawImage(vid, 0, 0);
      
      // Also draw to local preview canvas
      const localCanvas = document.getElementById('localCanvas');
      const localCtx = localCanvas.getContext('2d');
      localCanvas.width = vid.videoWidth;
      localCanvas.height = vid.videoHeight;
      localCtx.drawImage(vid, 0, 0, localCanvas.width, localCanvas.height);
      
      c.toBlob(blob => {
        const reader = new FileReader();
        reader.onloadend = () => {
          const b64 = reader.result.split(',')[1];
          ws.send(JSON.stringify({ type:'video_data', data:b64, timestamp:new Date().toISOString() }));
          addMessage(`Video frame sent (${blob.size} bytes)`, 'sent');
        };
        reader.readAsDataURL(blob);
      }, 'image/jpeg', 0.8);  // Add quality parameter
    }

    function toggleAudio() {
      if (audioRecorder) return stopAudio();
      
      // Request specific audio constraints
      const constraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000,  // Deepgram prefers 16kHz
          channelCount: 1     // Mono audio
        }
      };
      
      navigator.mediaDevices.getUserMedia(constraints)
        .then(stream => {
          audioStream = stream;
          
          // Check supported MIME types
          let mimeType = 'audio/webm';
          if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
            mimeType = 'audio/webm;codecs=opus';
          } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
            mimeType = 'audio/mp4';
          }
          
          audioRecorder = new MediaRecorder(stream, { mimeType });
          
          let chunkCount = 0;
          audioRecorder.ondataavailable = e => {
            if (e.data.size > 0) {
              const reader = new FileReader();
              reader.onloadend = () => {
                const b64 = reader.result.split(',')[1];
                if (ws.readyState === WebSocket.OPEN) {
                  ws.send(JSON.stringify({ type:'audio_data', data:b64, timestamp:new Date().toISOString() }));
                  chunkCount++;
                  document.getElementById('audioStatus').textContent = `Audio Status: Streaming (${chunkCount} chunks sent)`;
                  addMessage(`Audio chunk ${chunkCount} sent (${e.data.size} bytes, ${mimeType})`, 'sent');
                }
              };
              reader.readAsDataURL(e.data);
            }
          };
          
          audioRecorder.start(1000);  // Send chunks every 1 second
          document.getElementById('audioBtn').textContent = 'Stop Audio Stream';
          document.getElementById('audioStatus').textContent = 'Audio Status: Recording...';
          addMessage(`Audio stream started (${mimeType})`, 'info');
          
          // Monitor audio levels
          const audioContext = new AudioContext();
          const analyser = audioContext.createAnalyser();
          const microphone = audioContext.createMediaStreamSource(stream);
          microphone.connect(analyser);
          
          analyser.fftSize = 256;
          const bufferLength = analyser.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          
          function checkAudioLevel() {
            if (!audioRecorder || audioRecorder.state !== 'recording') return;
            
            analyser.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((a, b) => a + b) / bufferLength;
            
            if (average > 10) {  // Threshold for detecting audio
              document.getElementById('audioStatus').textContent = 
                `Audio Status: Streaming (${chunkCount} chunks sent) - Level: ${Math.round(average)}`;
            }
            
            requestAnimationFrame(checkAudioLevel);
          }
          checkAudioLevel();
          
        })
        .catch(e => {
          addMessage('Mic error: ' + e.message, 'error');
          document.getElementById('audioStatus').textContent = 'Audio Status: Error - ' + e.message;
        });
    }

    function stopAudio() {
      if (audioRecorder) {
        audioRecorder.stop();
        audioStream.getTracks().forEach(t => t.stop());
        audioRecorder = null;
        document.getElementById('audioBtn').textContent = 'Start Audio Stream';
        document.getElementById('audioStatus').textContent = 'Audio Status: Stopped';
        addMessage('Audio stream stopped', 'info');
      }
    }

    function stopMedia() {
      if (audioRecorder) stopAudio();
      clearInterval(videoTimer);
      const vid = document.getElementById('localVideo');
      if (vid.srcObject) vid.srcObject.getTracks().forEach(t=>t.stop());
    }

    function sendPing() {
      ws.send(JSON.stringify({ type:'ping', timestamp:new Date().toISOString() }));
    }
    function sendConfig() {
      const frequency = document.getElementById('videoFreq').value;
      ws.send(JSON.stringify({
        type:'config',
        data:{ video_frequency: frequency },
        timestamp:new Date().toISOString()
      }));
      
      // Apply new frequency immediately
      if (videoTimer) {
        clearInterval(videoTimer);
        videoTimer = setInterval(captureAndSendFrame, parseDuration(frequency));
        addMessage(`Video frequency updated to ${frequency}`, 'info');
      }
    }
    function stopSession() {
      ws.send(JSON.stringify({ type:'stop', timestamp:new Date().toISOString() }));
    }
    function disconnect() {
      ws.close();
    }
    function updateStatus(txt, ok) {
      const st = document.getElementById('status');
      st.textContent = txt;
      st.className = 'status ' + (ok?'connected':'disconnected');
    }
    function clearMessages() {
      document.getElementById('messages').innerHTML = '';
      addMessage('Messages cleared', 'info');
    }
    
    // Track message count to prevent too many messages
    let messageCount = 0;
    const MAX_MESSAGES = 100;
    
    function addMessage(txt, cls='msg') {
      const m = document.createElement('div');
      m.className = 'message '+cls;
      m.innerText = `[${new Date().toLocaleTimeString()}] ${txt}`;
      
      const messagesDiv = document.getElementById('messages');
      messagesDiv.appendChild(m);
      messageCount++;
      
      // Auto-prune if too many messages
      if (messageCount > MAX_MESSAGES) {
        // Remove the oldest message
        if (messagesDiv.firstChild) {
          messagesDiv.removeChild(messagesDiv.firstChild);
        }
        messageCount--;
      }
      
      // Auto-scroll to bottom
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }
    function parseDuration(s) {
      // "5s" → 5000
      return Number(s.replace('s','')) * 1000;
    }
  </script>
</body>
</html>
